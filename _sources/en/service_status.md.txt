# Service Status

Welcome to Evam Status page. Here you'll find live and historical data on system performance.


## UK 

[Subscribe to Incident Updates](https://cdn.forms-content-1.sg-form.com/85a32dd5-8cad-11f0-b30a-161b0ada2de2)


### Active Incidents

`````{note}
All Systems Operational
`````





### Historical incidents
Below you'll find a record of all incidents from the past 30 days.

>### Central Services Full Outage
>
>**Severity:** S1-Critical [Read more](#incident-categorisation)
>
>**Affected Services**: Central Services Cloud Gateway and all services connecting to it e.g. Central Services and Vehicle Services
>
>### Incident Updates
>
>**Investigating:** We are currently investigating the issue and working to resolve it as quickly as possible. 
>
>*Posted: 28 January 2026, 15:38 UTC*
>
>**Monitoring:** A fix has been implemented, and we are closely monitoring to ensure everything is stable.
>
>*Posted: 28 January 2026, 16:25 UTC*
>
>**Resolved:** The issue has been resolved, and all systems are operating normally.
>
>*Posted: 28 January 2026, 16:49 UTC*
>
> ## Post-incident report
>
> **Post-incident report posted:** 12 February 2026
>
> \-
>
> **Date:** 28 January 2026\
> **Duration:** 47 minutes (15:38–16:25 UTC)\
> **Impact:** **Full outage** for all users in the UK organisation — users could not use key functions such as mobilising, sending status updates, chat messaging, and related features.
>
> ### Summary
>
> During our planned monthly maintenance, we shut down one of our core application instances as part of a standard, one-by-one update process. At this point in time our load balancer should have moved away traffic from the instance that was shut down to another healthy instance. However, traffic did not shift to the remaining healthy instance as quickly as expected, and the remaining instance could not fully operate due to an internal dependency involved in handling user requests. This combination turned what should have been a partial disruption into a full outage.
>
> ### Timeline (UTC)
>
> * **15:38** – Outage began
> * **15:57** – Response started
> * **16:25** – Service restored
> * **16:49** – Recovery verified
>
> ### Detection and alerts
>
> No automatic alert triggered at the start of the incident because alerts were temporarily muted due to maintenance on the alerting system shortly beforehand. This delayed our response by around **15 minutes** hence, responce was only started at 15:57.
>
> ### Resolution
>
> We followed our recovery procedures and restored service by bringing the affected instance(s) back online and confirming normal operation through monitoring and verification checks.
>
> ### Root cause
>
> The platform load balancer is designed to re-route traffic away from an instance that is taken offline, but this incident has uncovered that it does not guarantee the switch happens within a specific short time window. This is despite the configuration of the service beeing correct. During this maintenance sequence, traffic did not move over in time, and the remaining instance could not fully handle requests due to an internal dependency path also caused by the load balancer not acting as intended. Together, these factors caused a complete outage. Previous tests and maintenance has shown that this process should have worked and no impact on the availability of the services was expected.
>
> ### Preventing recurrence
>
> * **Keep monitoring fully available during production maintenance:** we will separate production maintenance from monitoring/alerting maintenance so alerts are not impacted.
> * **Reduce disruptive maintenance actions:** we will reduce hard reboots in production environments unless required for critical security updates. This will happen until longer-term resilience improvements has been deployed.
> * **Longer-term resilience improvement:** ongoing work to migrate this infrastructure to a more resilient platform (internal tracking: #2601).
>
> ### Apology
>
> We’re sorry for the disruption this caused to UK users. We’re taking the steps above to reduce the likelihood of a repeat and to improve detection and communication speed.
>
> If you have any further questions, please reach out to support@evam.life


## Sweden
[Subscribe to Incident Updates](https://cdn.forms-content-1.sg-form.com/b8bdae67-8c99-11f0-b323-163051fa07b0)

### Active Incidents


`````{note}
All Systems Operational
`````



### Historical incidents
Below you'll find a record of all incidents from the past 30 days.